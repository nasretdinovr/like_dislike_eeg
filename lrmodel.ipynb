{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import pandas\n",
    "import Queue\n",
    "# import pyedflib\n",
    "from os import walk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = \"data\"\n",
    "files = []\n",
    "data_set_t = []\n",
    "target_t = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(dir):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "for instance, file  in enumerate(files):\n",
    "    if (file == \"T0.npy\" or file == \"T1.npy\" or file == \"T2.npy\" or file == \"T3.npy\" or file == \"T4.npy\" or file == \"T5.npy\"):\n",
    "        continue\n",
    "    if (file == \"0.npy\" or file == \"1.npy\" or file == \"2.npy\" or file == \"3.npy\" or file == \"4.npy\" or file == \"5.npy\"):\n",
    "        data_set_t.append(np.load(dir + '/' +file))\n",
    "        target_t.append(np.load(dir + '/T' +file))\n",
    "calm_test= np.load(dir + '/calm_test.npy')\n",
    "calm_train = np.load(dir + '/calm_train.npy')\n",
    "\n",
    "Tcalm_test= np.load(dir + '/Tcalm_test.npy')\n",
    "Tcalm_train = np.load(dir + '/Tcalm_train.npy')\n",
    "\n",
    "data_nsh = np.vstack((data_set_t[0],data_set_t[1],data_set_t[2],data_set_t[3],data_set_t[4], data_set_t[5]))\n",
    "targets_nsh = np.hstack((target_t[0],target_t[1],target_t[2],target_t[3],target_t[4],  target_t[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_nsh, targets_nsh, test_size=0.03, shuffle = True)\n",
    "\n",
    "data_set = np.vstack((X_train, calm_train))\n",
    "targets = np.hstack((y_train, Tcalm_train))\n",
    "data_set = np.transpose(data_set, (0, 2, 1))\n",
    "\n",
    "b_targets[55:75] = np.vstack((calm_test[120:248], X_test))\n",
    "val_targs = np.hstack((Tcalm_test[120:248], y_test))\n",
    "val_data = np.transpose(val_data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del data_nsh\n",
    "del targets_nsh\n",
    "#del calm_test\n",
    "#del calm_train\n",
    "#del Tcalm_test\n",
    "#del Tcalm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, targets, batch_size):\n",
    "    indexes = np.arange(dataset.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    dataset = dataset[indexes]\n",
    "    targets = targets[indexes]\n",
    "    \n",
    "    tr_data = np.zeros((dataset.shape[0]/batch_size, batch_size, 31, 500))   \n",
    "    tr_targs = np.zeros((dataset.shape[0]/batch_size, batch_size))\n",
    "    \n",
    "    for i in range (dataset.shape[0]/batch_size):\n",
    "        tr_data[i] = copy.deepcopy(dataset[i*batch_size:(i+1)*batch_size])\n",
    "        tr_targs[i] = copy.deepcopy(targets[i*batch_size: (i+1)*batch_size])\n",
    "    return tr_data, tr_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LikeDislikeModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_size,\n",
    "                 L_sizes = [300, 200, 300],\n",
    "                 net = None,\n",
    "                 fc_sizes = [700, 250],\n",
    "                 n_local_pred = 3,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [256, 128, 64], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True):\n",
    "        \n",
    "        super(LikeDislikeModel, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.fc_sizes = fc_sizes\n",
    "        self.out_size = out_size\n",
    "        self.n_local_pred = n_local_pred\n",
    "        self.use_cuda = use_cuda\n",
    "        self.L_sizes = L_sizes\n",
    "        self.net = net\n",
    "        \n",
    "        self.f_sizes = f_sizes\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.len_sizes = self._len_sizes()\n",
    "        \n",
    "        self.conv1 = nn.ModuleList()\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        self.conv3 = nn.ModuleList()\n",
    "        self.fc1 = nn.ModuleList()\n",
    "        self.fc2 = nn.ModuleList()\n",
    "        self.fc3 = nn.ModuleList()\n",
    "        \n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range (n_local_pred):\n",
    "            \n",
    "            self.conv1.append(nn.Conv1d(in_channels, channels[0], f_sizes[0], strides[0]))\n",
    "            self.conv2.append(nn.Conv1d(channels[0], channels[1], f_sizes[1], strides[1]))               \n",
    "            self.conv3.append(nn.Conv1d(channels[1], channels[2], f_sizes[2], strides[2]))\n",
    "            \n",
    "            self.fc1.append(nn.Linear(self.len_sizes[i][-1] * self.channels[-1], fc_sizes[0]))\n",
    "            self.fc2.append(nn.Linear(fc_sizes[0], fc_sizes[1]))\n",
    "            self.fc3.append(nn.Linear(fc_sizes[1], out_size))\n",
    "            self.batchnorm.append(nn.ModuleList())\n",
    "            for convNum in range(3):\n",
    "                self.batchnorm[i].append(nn.BatchNorm1d(channels[convNum]))\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def _len_sizes(self):\n",
    "        len_sizes = []\n",
    "        for i in range (self.n_local_pred):\n",
    "            temp = []\n",
    "            for j in range (len(self.f_sizes)):\n",
    "                if j == 0:\n",
    "                    temp.append(int(math.floor((self.L_sizes[i] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "                else:\n",
    "                    temp.append(int(math.floor((temp[j-1] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "            len_sizes.append(copy.deepcopy(temp))\n",
    "        return len_sizes\n",
    "\n",
    "    def preprocessing(self, input_batch):\n",
    "        nns = []\n",
    "        if (self.n_local_pred == 1):\n",
    "            if (self.net == 0):\n",
    "                nns.append(input_batch[:, :, 200:500])\n",
    "            elif (self.net == 1):\n",
    "                nns.append(input_batch[:, :, 500:700])\n",
    "            elif (self.net == 2):\n",
    "                nns.append(input_batch[:, :, 700:1000])\n",
    "            elif (self.net == 3):\n",
    "                nns.append(input_batch)\n",
    "            else:\n",
    "                nns.append(input_batch[:, :, 200:1000])\n",
    "        else:\n",
    "            nns.append(input_batch[:, :, 200:500])\n",
    "            nns.append(input_batch[:, :, 500:700])\n",
    "            nns.append(input_batch[:, :, 700:1000])\n",
    "        #print (nns[0].shape, len(nns))\n",
    "        for i in range (self.n_local_pred):\n",
    "            zero_mean = nns[i] - nns[i].mean()\n",
    "            for n in range(input_batch.shape[0]):\n",
    "                nns[i][n] = zero_mean[n, :, :]/(np.std(zero_mean[n, :, :],axis = 0))\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i]))).cuda()\n",
    "            else:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i])))\n",
    "        return nns\n",
    "\n",
    "    def createLocalNetworks(self, nns):\n",
    "        raw_output = []\n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            conv1 = self.conv1[nn_num](nns[nn_num])\n",
    "            bn1 = self.batchnorm[nn_num][0](conv1)\n",
    "            relu1 = F.relu(bn1)\n",
    "\n",
    "            conv2 = self.conv2[nn_num](relu1)\n",
    "            bn2 = self.batchnorm[nn_num][1](conv2)\n",
    "            relu2 = F.relu(bn2)\n",
    "            \n",
    "            conv3 = self.conv3[nn_num](relu2)\n",
    "            bn3 = self.batchnorm[nn_num][2](conv3)\n",
    "            relu3 = F.relu(bn3)\n",
    "            \n",
    "            relu3 = relu3.view(self.batch_size, -1)\n",
    "            #print (relu3.size(), self.len_sizes[i][-1] * self.channels[-1]) \n",
    "            #print (self.len_sizes[0], len(self.len_sizes))\n",
    "            \n",
    "            fc1 = self.fc1[nn_num](relu3)\n",
    "            relu4 = F.relu(fc1)\n",
    "            do1 = self.dropout(relu4)\n",
    "            \n",
    "            fc2 = self.fc2[nn_num](do1)\n",
    "            relu5 = F.relu(fc2)            \n",
    "\n",
    "            do2 = self.dropout(relu5)\n",
    "\n",
    "            fc3 = self.fc3[nn_num](do2)\n",
    "            \n",
    "            raw_output.append(F.log_softmax(fc3, dim = 1))\n",
    "        return raw_output\n",
    "    \n",
    "    def _loss(self, raw_output, targets):\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "            \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "        scores = scores/self.n_local_pred\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            targets = Variable(torch.LongTensor(targets)).cuda()\n",
    "        else:\n",
    "            targets = Variable(torch.LongTensor(targets))\n",
    "            \n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        return loss\n",
    "    \n",
    "    def accurancy(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "        \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "        scores = scores/self.n_local_pred\n",
    "            \n",
    "        _, index = torch.max(scores,1)\n",
    "        index = index.cpu().data.numpy()\n",
    "        acc = (index == targets).mean()\n",
    "        return acc\n",
    "    \n",
    "    def forward(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        loss = self._loss(raw_output, targets)\n",
    "        return loss\n",
    "   \n",
    "    def predict(self, input_batch):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "        \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "        scores = scores/self.n_local_pred\n",
    "            \n",
    "        _, index = torch.max(scores,1)\n",
    "        index = index.cpu().data.numpy()\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir_root = './logdir'\n",
    "model = LikeDislikeModel(31, 3, L_sizes = [500],\n",
    "                 net = 3,\n",
    "                 fc_sizes = [7500, 1500],\n",
    "                 n_local_pred = 1,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [256, 128, 64], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = False)\n",
    "if model.use_cuda:\n",
    "    model.cuda()\n",
    "model.load_state_dict(torch.load('cpu_model'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 2 1 2 0 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 1 2 2 1 2 2 1 2\n",
      " 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 1 2\n",
      " 2 2 2 1 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 0 2 2\n",
      " 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 2 2]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  2.  2.  2.  2.  2.  1.  2.  2.  1.  1.  2.  1.  2.  1.  1.\n",
      "  2.  1.  1.  1.  2.  2.  2.  1.  2.  1.  1.  1.  1.  1.  2.  1.  2.  1.\n",
      "  2.  2.  1.  1.  2.  1.  2.  1.  1.  2.  2.  2.  1.  2.  2.  2.  2.  2.\n",
      "  2.  1.  1.  2.  2.  2.  1.  2.  1.  2.  2.  2.  2.  1.  2.  1.  2.  1.\n",
      "  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  1.  1.  2.  1.  2.\n",
      "  1.  2.  1.  2.  2.  1.  2.  2.  2.  2.  2.  1.  1.  2.  1.  2.  2.  2.\n",
      "  1.  2.  2.  1.  2.  2.  1.  2.  1.  1.  2.  2.  2.  2.  1.  2.  1.  2.\n",
      "  2.  1.  2.  2.  1.  2.  2.  1.  2.  2.  2.  1.  2.  2.  1.  2.  1.  1.\n",
      "  1.  1.  2.  1.  2.  1.  1.  1.  2.  2.  2.  1.  2.  2.  2.  1.  1.  2.\n",
      "  2.  2.  2.  1.  1.  1.  1.  2.  1.  1.  2.  2.  2.  1.  1.  2.  2.  2.\n",
      "  2.  2.  1.  1.  1.  2.  1.  2.  1.  1.  2.  1.  2.  2.  2.  2.  2.  1.\n",
      "  2.  2.  2.  1.  2.  1.  2.  1.  1.  1.  2.  2.  2.  1.  2.  1.  2.  1.\n",
      "  2.  2.  1.  2.  2.  2.  2.  1.  1.  2.  2.  2.  1.  1.  2.  1.  1.  1.\n",
      "  2.  2.  1.  1.  2.  1.]\n"
     ]
    }
   ],
   "source": [
    "print( model.predict(np.transpose(calm_test[120:248], (0, 2, 1))))\n",
    "print (val_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 0 2 2 2 0 2 2 0 0 0 2 2 2 0 1 2 0]\n",
      "[ 1.  1.  2.  0.  2.  2.  2.  0.  1.  1.  0.  0.  0.  1.  2.  2.  0.  2.\n",
      "  1.  0.]\n"
     ]
    }
   ],
   "source": [
    "tr_data, tr_targs = get_data(data_set, targets, 256)\n",
    "\n",
    "batch = tr_data[0]\n",
    "b_targets = tr_targs[0]\n",
    "\n",
    "\n",
    "print (model.predict(batch[55:75]))\n",
    "print (b_targets[55:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = model.cpu()\n",
    "# torch.save(model.state_dict(), './cpu_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000006, weight_decay = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logdir_root = './logdir2'\n",
    "#ValLossFile = open(logdir_root+'/val_loss.txt', 'w')\n",
    "\n",
    "iterations = data_set.shape[0] / 256\n",
    "epochs = 1000\n",
    "\n",
    "#log_loss = np.zeros(epochs*iterations)\n",
    "block = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tr_data, tr_targs = get_data(data_set, targets, 256)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        batch = tr_data[iteration]\n",
    "        b_targets = tr_targs[iteration]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model(batch, b_targets)\n",
    "        #log_loss[epoch*iterations + iteration] = loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    \n",
    "    if model.use_cuda:\n",
    "        v_acc = model.accurancy(val_data, val_targs)\n",
    "        acc =  model.accurancy(batch, b_targets)\n",
    "        v_loss = model(val_data, val_targs)\n",
    "        \n",
    "        print('epoch {:d}, loss = {:.3f}, val_loss = {:.3f}, val_acc = {:.3f}, train_acc = {:.3f}'\n",
    "              .format(epoch, loss.cpu().data[0], v_loss.cpu().data[0], v_acc, acc))\n",
    "        #ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.cpu().data[0]), 'w')\n",
    "        #ValLossFile.write('{:.3f} {:.3f}\\n'.format(acc, v_acc))\n",
    "    else:\n",
    "        print('block {:d}, loss = {:.3f}'\n",
    "              .format(block, loss.data[0]))\n",
    "        ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.data[0]), 'w')\n",
    "        #ValLossFile.write('{:.3f} {:.3f}\\n'.format(loss.data[0], v_loss.data[0]))\n",
    "   # block += 1\n",
    "\n",
    "    #torch.save(model.state_dict(), ModelFile)\n",
    "    #ModelFile.close()\n",
    "#np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 62, 1200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.zeros((256, 62, 1200))\n",
    "for i in range (tst_data.shape[0]):\n",
    "    test[i] = tst_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nns = model.preprocessing(batch)\n",
    "raw_output = model.createLocalNetworks(nns)\n",
    "scores = Variable(torch.FloatTensor(model.batch_size, model.out_size).zero_()).cuda()\n",
    "for nn_num in range( model.n_local_pred):\n",
    "    scores = scores + raw_output[nn_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3])\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "#scores = scores[:tst_data.shape[0]]\n",
    "\n",
    "print (scores.shape)\n",
    "_, index = torch.max(scores,1)\n",
    "index = index.cpu().data.numpy()\n",
    "print ((index == b_targets).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "print (model.accurancy(np.transpose(calm_test[400:600],(0,2,1)),np.zeros((200))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
