{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import pandas\n",
    "import Queue\n",
    "# import pyedflib\n",
    "from os import walk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6199917269217241025\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10741206221\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 6036455015682469890\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10972997223\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 15832592981884255668\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = \"np_data13\"\n",
    "files = []\n",
    "data_set_t = []\n",
    "target_t = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(dir):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "for instance, file  in enumerate(files):\n",
    "    if (file == \"T0.npy\" or file == \"T1.npy\" or file == \"T2.npy\" or file == \"T3.npy\" or file == \"T4.npy\" or file == \"T5.npy\"):\n",
    "        continue\n",
    "    if (file == \"0.npy\" or file == \"1.npy\" or file == \"2.npy\" or file == \"3.npy\" or file == \"4.npy\" or file == \"5.npy\"):\n",
    "        data_set_t.append(np.load(dir + '/' +file))\n",
    "        target_t.append(np.load(dir + '/T' +file))\n",
    "calm_test= np.load(dir + '/calm_test.npy')\n",
    "calm_train = np.load(dir + '/calm_train.npy')\n",
    "\n",
    "Tcalm_test= np.load(dir + '/Tcalm_test.npy')\n",
    "Tcalm_train = np.load(dir + '/Tcalm_train.npy')\n",
    "\n",
    "data_nsh = np.vstack((data_set_t[0],data_set_t[1],data_set_t[2],data_set_t[3],data_set_t[4], data_set_t[5]))\n",
    "targets_nsh = np.hstack((target_t[0],target_t[1],target_t[2],target_t[3],target_t[4],  target_t[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_nsh, targets_nsh, test_size=0.03, shuffle = True)\n",
    "\n",
    "data_set = np.vstack((X_train, calm_train))\n",
    "targets = np.hstack((y_train, Tcalm_train))\n",
    "# data_set = np.transpose(data_set, (0, 2, 1))\n",
    "\n",
    "val_data = np.vstack((calm_test[120:248], X_test))\n",
    "val_targs = np.hstack((Tcalm_test[120:248], y_test))\n",
    "# val_data = np.transpose(val_data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test\n",
    "del y_train\n",
    "del y_test\n",
    "del data_nsh\n",
    "del targets_nsh\n",
    "gc.collect()\n",
    "del calm_test\n",
    "del calm_train\n",
    "del Tcalm_test\n",
    "del Tcalm_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, targets, batch_size):\n",
    "    indexes = np.arange(dataset.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    dataset = dataset[indexes]\n",
    "    targets = targets[indexes]\n",
    "    \n",
    "    tr_data = np.zeros((dataset.shape[0]/batch_size, batch_size, 31, 500))   \n",
    "    tr_targs = np.zeros((dataset.shape[0]/batch_size, batch_size))\n",
    "    \n",
    "    for i in range (dataset.shape[0]/batch_size):\n",
    "        tr_data[i] = copy.deepcopy(dataset[i*batch_size:(i+1)*batch_size])\n",
    "        tr_targs[i] = copy.deepcopy(targets[i*batch_size: (i+1)*batch_size])\n",
    "    return tr_data, tr_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LikeDislikeModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_size,\n",
    "                 L_sizes = [300, 200, 300],\n",
    "                 net = None,\n",
    "                 fc_sizes = [700, 250],\n",
    "                 n_local_pred = 3,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [256, 128, 64], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True):\n",
    "        \n",
    "        super(LikeDislikeModel, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.fc_sizes = fc_sizes\n",
    "        self.out_size = out_size\n",
    "        self.n_local_pred = n_local_pred\n",
    "        self.use_cuda = use_cuda\n",
    "        self.L_sizes = L_sizes\n",
    "        self.net = net\n",
    "        \n",
    "        self.f_sizes = f_sizes\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.len_sizes = self._len_sizes()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels, channels[0], f_sizes[0], strides[0])\n",
    "        self.conv2 = nn.Conv1d(channels[0], channels[1], f_sizes[1], strides[1])              \n",
    "        self.conv3 = nn.Conv1d(channels[1], channels[2], f_sizes[2], strides[2])\n",
    "\n",
    "        self.fc1 = nn.Linear(self.len_sizes[-1] * self.channels[-1], fc_sizes[0])\n",
    "        self.fc2 = nn.Linear(fc_sizes[0], fc_sizes[1])\n",
    "        self.fc3 = nn.Linear(fc_sizes[1], out_size)\n",
    "        \n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        for convNum in range(3):\n",
    "            self.batchnorm.append(nn.BatchNorm1d(channels[convNum]))\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def _len_sizes(self):\n",
    "        len_sizes = []\n",
    "\n",
    "        temp = []\n",
    "        for j in range (len(self.f_sizes)):\n",
    "            if j == 0:\n",
    "                temp.append(int(math.floor((self.L_sizes - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "            else:\n",
    "                temp.append(int(math.floor((temp[j-1] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "\n",
    "        \n",
    "        return temp\n",
    "\n",
    "    def preprocessing(self, input_batch):\n",
    "        #zero_mean = input_batch - input_batch.mean()\n",
    "        \"\"\"     \n",
    "        inp = np.zeros((input_batch.shape))\n",
    "        for n in range(input_batch.shape[0]):\n",
    "            zero_mean = input_batch[n] - input_batch[n].mean()\n",
    "            \n",
    "            inp[n] = zero_mean/(np.std(zero_mean,axis = 0))\n",
    "        del input_batch\n",
    "        del zero_mean\"\"\"\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            input_batch = Variable(torch.FloatTensor(input_batch)).cuda()\n",
    "        else:\n",
    "            input_batch = Variable(torch.FloatTensor(input_batch))\n",
    "        #print (input_batch[0])\n",
    "        return input_batch\n",
    "\n",
    "    def createLocalNetworks(self, nn):\n",
    "\n",
    "        conv1 = self.conv1(nn)\n",
    "        bn1 = self.batchnorm[0](conv1)\n",
    "        #print (conv1)\n",
    "        relu1 = F.relu(bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        bn2 = self.batchnorm[1](conv2)\n",
    "        relu2 = F.relu(bn2)\n",
    "\n",
    "        conv3 = self.conv3(relu2)\n",
    "        bn3 = self.batchnorm[2](conv3)\n",
    "        relu3 = F.relu(bn3)\n",
    "\n",
    "        relu3 = relu3.view(self.batch_size, -1)\n",
    "\n",
    "        fc1 = self.fc1(relu3)\n",
    "        relu4 = F.relu(fc1)\n",
    "        do1 = self.dropout(relu4)\n",
    "\n",
    "        fc2 = self.fc2(do1)\n",
    "        relu5 = F.relu(fc2)            \n",
    "\n",
    "        do2 = self.dropout(relu5)\n",
    "\n",
    "        fc3 = self.fc3(do2)\n",
    "            \n",
    "        return fc3\n",
    "    \n",
    "    def net_pred(self, nn):\n",
    "        \n",
    "        conv1 = self.conv1(nn)\n",
    "        bn1 = self.batchnorm[0](conv1)\n",
    "        relu1 = F.relu(bn1)\n",
    "\n",
    "        conv2 = self.conv2(relu1)\n",
    "        bn2 = self.batchnorm[1](conv2)\n",
    "        relu2 = F.relu(bn2)\n",
    "\n",
    "        conv3 = self.conv3(relu2)\n",
    "        bn3 = self.batchnorm[2](conv3)\n",
    "        relu3 = F.relu(bn3)\n",
    "\n",
    "        relu3 = relu3.view(self.batch_size, -1)\n",
    "\n",
    "        fc1 = self.fc1(relu3)\n",
    "        relu4 = F.relu(fc1)\n",
    "        \n",
    "        \n",
    "        fc2 = self.fc2(relu4)\n",
    "        relu5 = F.relu(fc2)            \n",
    "\n",
    "        fc3 = self.fc3(relu5)\n",
    "            \n",
    "        return fc3\n",
    "    \n",
    "    def _loss(self, raw_output, targets):\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            targets = Variable(torch.LongTensor(targets)).cuda()\n",
    "        else:\n",
    "            targets = Variable(torch.LongTensor(targets))\n",
    "            \n",
    "        loss = F.cross_entropy(raw_output, targets)\n",
    "        return loss\n",
    "    \n",
    "    def accurancy(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.net_pred(nns)\n",
    "        #raw_output = self.createLocalNetworks(input_batch)\n",
    "        \n",
    "        _, index = torch.max(raw_output,1)\n",
    "        index = index.cpu().data.numpy()\n",
    "        acc = (index == targets).mean()\n",
    "        return acc\n",
    "    \n",
    "    def forward(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        #raw_output = self.createLocalNetworks(input_batch)\n",
    "        loss = self._loss(raw_output, targets)\n",
    "        return loss\n",
    "   \n",
    "    def predict(self, input_batch):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.net_pred(nns)\n",
    "        #raw_output = self.createLocalNetworks(input_batch)\n",
    "            \n",
    "        _, index = torch.max(raw_output,1)\n",
    "        index = index.cpu().data.numpy()\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir_root = './logdir2'\n",
    "model = LikeDislikeModel(31, 3, L_sizes = 500,\n",
    "                 net = 3,\n",
    "                 fc_sizes = [7500, 1500],\n",
    "                 n_local_pred = 1,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [256, 128, 64], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True)\n",
    "if model.use_cuda:\n",
    "    model.cuda()\n",
    "model.load_state_dict(torch.load(logdir_root + '/block0-loss=0.632_model.txt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.1229e-06 -2.2399e-06 -2.3691e-06  ...  -1.4273e-06 -1.5577e-06 -1.7114e-06\n",
      "-2.7088e-06 -2.8214e-06 -2.9426e-06  ...  -2.9461e-06 -2.8583e-06 -2.7758e-06\n",
      "-5.0312e-06 -5.1097e-06 -5.1855e-06  ...  -3.9228e-06 -4.2068e-06 -4.5097e-06\n",
      "                ...                   ⋱                   ...                \n",
      " 1.4884e-07  5.2590e-08 -4.8773e-08  ...   4.6439e-07  4.8611e-07  4.9790e-07\n",
      "-3.0315e-06 -3.1530e-06 -3.2895e-06  ...  -5.2069e-07 -1.2924e-07  2.2967e-07\n",
      "-2.5119e-06 -2.3377e-06 -2.1838e-06  ...   2.3019e-06  2.9117e-06  3.4292e-06\n",
      "[torch.cuda.FloatTensor of size 31x500 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  4.5850e-02  4.5849e-02  4.5849e-02  ...   4.5848e-02  4.5847e-02  4.5847e-02\n",
      "  6.1525e-02  6.1525e-02  6.1526e-02  ...   6.1526e-02  6.1526e-02  6.1526e-02\n",
      " -5.8780e-02 -5.8780e-02 -5.8780e-02  ...  -5.8781e-02 -5.8781e-02 -5.8781e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1091e-02  7.1091e-02  7.1091e-02  ...   7.1091e-02  7.1092e-02  7.1092e-02\n",
      " -6.6813e-03 -6.6812e-03 -6.6811e-03  ...  -6.6815e-03 -6.6815e-03 -6.6815e-03\n",
      " -2.3913e-02 -2.3913e-02 -2.3913e-02  ...  -2.3913e-02 -2.3913e-02 -2.3913e-02\n",
      "\n",
      "( 1 ,.,.) = \n",
      "  4.5849e-02  4.5850e-02  4.5850e-02  ...   4.5848e-02  4.5848e-02  4.5848e-02\n",
      "  6.1525e-02  6.1525e-02  6.1525e-02  ...   6.1525e-02  6.1525e-02  6.1526e-02\n",
      " -5.8782e-02 -5.8781e-02 -5.8781e-02  ...  -5.8782e-02 -5.8782e-02 -5.8782e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1092e-02  7.1092e-02  7.1092e-02  ...   7.1091e-02  7.1091e-02  7.1091e-02\n",
      " -6.6818e-03 -6.6816e-03 -6.6813e-03  ...  -6.6823e-03 -6.6824e-03 -6.6824e-03\n",
      " -2.3914e-02 -2.3914e-02 -2.3913e-02  ...  -2.3914e-02 -2.3914e-02 -2.3914e-02\n",
      "\n",
      "( 2 ,.,.) = \n",
      "  4.5851e-02  4.5852e-02  4.5853e-02  ...   4.5850e-02  4.5850e-02  4.5849e-02\n",
      "  6.1526e-02  6.1526e-02  6.1526e-02  ...   6.1526e-02  6.1526e-02  6.1526e-02\n",
      " -5.8778e-02 -5.8777e-02 -5.8777e-02  ...  -5.8780e-02 -5.8781e-02 -5.8781e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1091e-02  7.1091e-02  7.1091e-02  ...   7.1091e-02  7.1091e-02  7.1092e-02\n",
      " -6.6815e-03 -6.6815e-03 -6.6815e-03  ...  -6.6815e-03 -6.6815e-03 -6.6814e-03\n",
      " -2.3915e-02 -2.3915e-02 -2.3915e-02  ...  -2.3913e-02 -2.3913e-02 -2.3913e-02\n",
      "... \n",
      "\n",
      "(384,.,.) = \n",
      "  4.5848e-02  4.5847e-02  4.5847e-02  ...   4.5849e-02  4.5849e-02  4.5848e-02\n",
      "  6.1526e-02  6.1526e-02  6.1526e-02  ...   6.1525e-02  6.1525e-02  6.1525e-02\n",
      " -5.8783e-02 -5.8783e-02 -5.8784e-02  ...  -5.8783e-02 -5.8783e-02 -5.8783e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1090e-02  7.1090e-02  7.1090e-02  ...   7.1091e-02  7.1091e-02  7.1091e-02\n",
      " -6.6823e-03 -6.6823e-03 -6.6823e-03  ...  -6.6828e-03 -6.6827e-03 -6.6826e-03\n",
      " -2.3914e-02 -2.3914e-02 -2.3914e-02  ...  -2.3914e-02 -2.3914e-02 -2.3914e-02\n",
      "\n",
      "(385,.,.) = \n",
      "  4.5842e-02  4.5842e-02  4.5842e-02  ...   4.5846e-02  4.5846e-02  4.5847e-02\n",
      "  6.1522e-02  6.1522e-02  6.1523e-02  ...   6.1526e-02  6.1527e-02  6.1527e-02\n",
      " -5.8784e-02 -5.8784e-02 -5.8784e-02  ...  -5.8783e-02 -5.8783e-02 -5.8783e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1091e-02  7.1091e-02  7.1091e-02  ...   7.1093e-02  7.1093e-02  7.1092e-02\n",
      " -6.6834e-03 -6.6831e-03 -6.6826e-03  ...  -6.6786e-03 -6.6781e-03 -6.6777e-03\n",
      " -2.3915e-02 -2.3915e-02 -2.3914e-02  ...  -2.3907e-02 -2.3907e-02 -2.3907e-02\n",
      "\n",
      "(386,.,.) = \n",
      "  4.5850e-02  4.5850e-02  4.5850e-02  ...   4.5851e-02  4.5851e-02  4.5852e-02\n",
      "  6.1525e-02  6.1525e-02  6.1526e-02  ...   6.1525e-02  6.1525e-02  6.1525e-02\n",
      " -5.8780e-02 -5.8780e-02 -5.8779e-02  ...  -5.8778e-02 -5.8777e-02 -5.8777e-02\n",
      "                 ...                   ⋱                   ...                \n",
      "  7.1091e-02  7.1091e-02  7.1091e-02  ...   7.1091e-02  7.1091e-02  7.1090e-02\n",
      " -6.6815e-03 -6.6813e-03 -6.6812e-03  ...  -6.6816e-03 -6.6815e-03 -6.6814e-03\n",
      " -2.3913e-02 -2.3913e-02 -2.3913e-02  ...  -2.3914e-02 -2.3914e-02 -2.3914e-02\n",
      "[torch.cuda.FloatTensor of size 387x256x249 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.5351\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(val_data, val_targs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2\n",
      " 2 2 2 1 2 1 2 2 1 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 1 1 2 2 0 2 2 1 1 2 2 2 2 2 2 2\n",
      " 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 2 2 2 1 2 0 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 0 2 2 2\n",
      " 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 1 0 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 1 2 2 2 2 2 1 2\n",
      " 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'print (val_data.shape)\\n\\nfor i in range (300):\\n    print (model.predict(val_data[i].reshape(1, 31, 500)))'"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( model.predict(val_data[0:128]))\n",
    "print( model.predict(val_data))\n",
    "\"\"\"print (val_data.shape)\n",
    "\n",
    "for i in range (300):\n",
    "    print (model.predict(val_data[i].reshape(1, 31, 500)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(387, 31, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-2.1229e-06 -2.2399e-06 -2.3691e-06  ...  -1.4273e-06 -1.5577e-06 -1.7114e-06\n",
      "-2.7088e-06 -2.8214e-06 -2.9426e-06  ...  -2.9461e-06 -2.8583e-06 -2.7758e-06\n",
      "-5.0312e-06 -5.1097e-06 -5.1855e-06  ...  -3.9228e-06 -4.2068e-06 -4.5097e-06\n",
      "                ...                   ⋱                   ...                \n",
      " 1.4884e-07  5.2590e-08 -4.8773e-08  ...   4.6439e-07  4.8611e-07  4.9790e-07\n",
      "-3.0315e-06 -3.1530e-06 -3.2895e-06  ...  -5.2069e-07 -1.2924e-07  2.2967e-07\n",
      "-2.5119e-06 -2.3377e-06 -2.1838e-06  ...   2.3019e-06  2.9117e-06  3.4292e-06\n",
      "[torch.cuda.FloatTensor of size 31x500 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(val_data[0].reshape(1,31,500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([250, 31, 500])\n",
      "torch.Size([250, 256, 249])\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(val_data[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67183462532299743"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accurancy(val_data, val_targs)\n",
    "#val_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000006, weight_decay = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss = 1.097, val_loss = 1.098, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 1, loss = 1.098, val_loss = 1.098, val_acc = 0.354, train_acc = 0.355\n",
      "epoch 2, loss = 1.091, val_loss = 1.098, val_acc = 0.354, train_acc = 0.453\n",
      "epoch 3, loss = 1.093, val_loss = 1.099, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 4, loss = 1.097, val_loss = 1.099, val_acc = 0.354, train_acc = 0.367\n",
      "epoch 5, loss = 1.093, val_loss = 1.099, val_acc = 0.354, train_acc = 0.371\n",
      "epoch 6, loss = 1.093, val_loss = 1.099, val_acc = 0.354, train_acc = 0.367\n",
      "epoch 7, loss = 1.094, val_loss = 1.099, val_acc = 0.354, train_acc = 0.402\n",
      "epoch 8, loss = 1.094, val_loss = 1.100, val_acc = 0.354, train_acc = 0.312\n",
      "epoch 9, loss = 1.088, val_loss = 1.099, val_acc = 0.354, train_acc = 0.438\n",
      "epoch 10, loss = 1.087, val_loss = 1.099, val_acc = 0.354, train_acc = 0.438\n",
      "epoch 11, loss = 1.092, val_loss = 1.100, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 12, loss = 1.093, val_loss = 1.100, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 13, loss = 1.096, val_loss = 1.099, val_acc = 0.354, train_acc = 0.359\n",
      "epoch 14, loss = 1.086, val_loss = 1.100, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 15, loss = 1.089, val_loss = 1.100, val_acc = 0.354, train_acc = 0.367\n",
      "epoch 16, loss = 1.086, val_loss = 1.100, val_acc = 0.354, train_acc = 0.398\n",
      "epoch 17, loss = 1.088, val_loss = 1.100, val_acc = 0.354, train_acc = 0.359\n",
      "epoch 18, loss = 1.090, val_loss = 1.100, val_acc = 0.354, train_acc = 0.344\n",
      "epoch 19, loss = 1.093, val_loss = 1.101, val_acc = 0.354, train_acc = 0.359\n",
      "epoch 20, loss = 1.080, val_loss = 1.100, val_acc = 0.354, train_acc = 0.441\n",
      "epoch 21, loss = 1.097, val_loss = 1.100, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 22, loss = 1.093, val_loss = 1.100, val_acc = 0.354, train_acc = 0.348\n",
      "epoch 23, loss = 1.092, val_loss = 1.101, val_acc = 0.354, train_acc = 0.375\n",
      "epoch 24, loss = 1.093, val_loss = 1.101, val_acc = 0.354, train_acc = 0.344\n",
      "epoch 25, loss = 1.091, val_loss = 1.100, val_acc = 0.354, train_acc = 0.367\n",
      "epoch 26, loss = 1.083, val_loss = 1.101, val_acc = 0.354, train_acc = 0.402\n",
      "epoch 27, loss = 1.084, val_loss = 1.101, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 28, loss = 1.090, val_loss = 1.100, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 29, loss = 1.092, val_loss = 1.101, val_acc = 0.354, train_acc = 0.391\n",
      "epoch 30, loss = 1.091, val_loss = 1.100, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 31, loss = 1.090, val_loss = 1.101, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 32, loss = 1.087, val_loss = 1.101, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 33, loss = 1.091, val_loss = 1.100, val_acc = 0.354, train_acc = 0.367\n",
      "epoch 34, loss = 1.091, val_loss = 1.101, val_acc = 0.354, train_acc = 0.340\n",
      "epoch 35, loss = 1.094, val_loss = 1.100, val_acc = 0.354, train_acc = 0.379\n",
      "epoch 36, loss = 1.087, val_loss = 1.100, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 37, loss = 1.087, val_loss = 1.101, val_acc = 0.354, train_acc = 0.441\n",
      "epoch 38, loss = 1.084, val_loss = 1.101, val_acc = 0.354, train_acc = 0.383\n",
      "epoch 39, loss = 1.075, val_loss = 1.100, val_acc = 0.354, train_acc = 0.480\n",
      "epoch 40, loss = 1.082, val_loss = 1.101, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 41, loss = 1.099, val_loss = 1.101, val_acc = 0.354, train_acc = 0.336\n",
      "epoch 42, loss = 1.092, val_loss = 1.100, val_acc = 0.354, train_acc = 0.359\n",
      "epoch 43, loss = 1.094, val_loss = 1.100, val_acc = 0.354, train_acc = 0.355\n",
      "epoch 44, loss = 1.086, val_loss = 1.101, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 45, loss = 1.088, val_loss = 1.101, val_acc = 0.354, train_acc = 0.375\n",
      "epoch 46, loss = 1.088, val_loss = 1.101, val_acc = 0.354, train_acc = 0.379\n",
      "epoch 47, loss = 1.083, val_loss = 1.101, val_acc = 0.354, train_acc = 0.422\n",
      "epoch 48, loss = 1.098, val_loss = 1.101, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 49, loss = 1.080, val_loss = 1.101, val_acc = 0.354, train_acc = 0.391\n",
      "epoch 50, loss = 1.089, val_loss = 1.101, val_acc = 0.354, train_acc = 0.375\n",
      "epoch 51, loss = 1.094, val_loss = 1.101, val_acc = 0.354, train_acc = 0.395\n",
      "epoch 52, loss = 1.089, val_loss = 1.101, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 53, loss = 1.092, val_loss = 1.101, val_acc = 0.354, train_acc = 0.359\n",
      "epoch 54, loss = 1.086, val_loss = 1.101, val_acc = 0.354, train_acc = 0.410\n",
      "epoch 55, loss = 1.090, val_loss = 1.101, val_acc = 0.354, train_acc = 0.379\n",
      "epoch 56, loss = 1.088, val_loss = 1.101, val_acc = 0.354, train_acc = 0.379\n",
      "epoch 57, loss = 1.094, val_loss = 1.101, val_acc = 0.354, train_acc = 0.336\n",
      "epoch 58, loss = 1.089, val_loss = 1.101, val_acc = 0.354, train_acc = 0.402\n",
      "epoch 59, loss = 1.093, val_loss = 1.101, val_acc = 0.354, train_acc = 0.359\n",
      "epoch 60, loss = 1.094, val_loss = 1.101, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 61, loss = 1.092, val_loss = 1.101, val_acc = 0.354, train_acc = 0.348\n",
      "epoch 62, loss = 1.079, val_loss = 1.101, val_acc = 0.354, train_acc = 0.430\n",
      "epoch 63, loss = 1.085, val_loss = 1.101, val_acc = 0.354, train_acc = 0.371\n",
      "epoch 64, loss = 1.086, val_loss = 1.101, val_acc = 0.354, train_acc = 0.438\n",
      "epoch 65, loss = 1.089, val_loss = 1.101, val_acc = 0.354, train_acc = 0.383\n",
      "epoch 66, loss = 1.083, val_loss = 1.101, val_acc = 0.354, train_acc = 0.406\n",
      "epoch 67, loss = 1.082, val_loss = 1.101, val_acc = 0.354, train_acc = 0.422\n",
      "epoch 68, loss = 1.091, val_loss = 1.101, val_acc = 0.354, train_acc = 0.391\n",
      "epoch 69, loss = 1.100, val_loss = 1.101, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 70, loss = 1.082, val_loss = 1.101, val_acc = 0.354, train_acc = 0.422\n",
      "epoch 71, loss = 1.087, val_loss = 1.102, val_acc = 0.354, train_acc = 0.379\n",
      "epoch 72, loss = 1.087, val_loss = 1.102, val_acc = 0.354, train_acc = 0.398\n",
      "epoch 73, loss = 1.084, val_loss = 1.101, val_acc = 0.354, train_acc = 0.438\n",
      "epoch 74, loss = 1.085, val_loss = 1.101, val_acc = 0.354, train_acc = 0.402\n",
      "epoch 75, loss = 1.098, val_loss = 1.101, val_acc = 0.354, train_acc = 0.352\n",
      "epoch 76, loss = 1.083, val_loss = 1.101, val_acc = 0.354, train_acc = 0.398\n",
      "epoch 77, loss = 1.089, val_loss = 1.101, val_acc = 0.354, train_acc = 0.379\n",
      "epoch 78, loss = 1.087, val_loss = 1.101, val_acc = 0.354, train_acc = 0.410\n",
      "epoch 79, loss = 1.086, val_loss = 1.102, val_acc = 0.354, train_acc = 0.387\n",
      "epoch 80, loss = 1.095, val_loss = 1.101, val_acc = 0.354, train_acc = 0.371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-549-617b2115b1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;31m#log_loss[epoch*iterations + iteration] = loss.data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-546-738381788de4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_batch, targets)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mnns\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateLocalNetworks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;31m#raw_output = self.createLocalNetworks(input_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-546-738381788de4>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCudaTransfer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/torch/autograd/_functions/tensor.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, device, async)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/torch/_utils.pyc\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logdir_root = './logdir2'\n",
    "#ValLossFile = open(logdir_root+'/val_loss.txt', 'w')\n",
    "\n",
    "iterations = data_set.shape[0] / 256\n",
    "epochs = 1000\n",
    "\n",
    "#log_loss = np.zeros(epochs*iterations)\n",
    "block = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tr_data, tr_targs = get_data(data_set, targets, 256)\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        batch = tr_data[iteration]\n",
    "        b_targets = tr_targs[iteration]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model(batch, b_targets)\n",
    "        #log_loss[epoch*iterations + iteration] = loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    \n",
    "    if model.use_cuda:\n",
    "        v_acc = model.accurancy(val_data, val_targs)\n",
    "        acc =  model.accurancy(batch, b_targets)\n",
    "        v_loss = model(val_data, val_targs)\n",
    "        \n",
    "        print('epoch {:d}, loss = {:.3f}, val_loss = {:.3f}, val_acc = {:.3f}, train_acc = {:.3f}'\n",
    "              .format(epoch, loss.cpu().data[0], v_loss.cpu().data[0], v_acc, acc))\n",
    "        #ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.cpu().data[0]), 'w')\n",
    "        #ValLossFile.write('{:.3f} {:.3f}\\n'.format(acc, v_acc))\n",
    "    else:\n",
    "        print('block {:d}, loss = {:.3f}'\n",
    "              .format(block, loss.data[0]))\n",
    "        ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.data[0]), 'w')\n",
    "        #ValLossFile.write('{:.3f} {:.3f}\\n'.format(loss.data[0], v_loss.data[0]))\n",
    "   # block += 1\n",
    "\n",
    "    #torch.save(model.state_dict(), ModelFile)\n",
    "    #ModelFile.close()\n",
    "#np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 62, 1200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.zeros((256, 62, 1200))\n",
    "for i in range (tst_data.shape[0]):\n",
    "    test[i] = tst_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nns = model.preprocessing(batch)\n",
    "raw_output = model.createLocalNetworks(nns)\n",
    "scores = Variable(torch.FloatTensor(model.batch_size, model.out_size).zero_()).cuda()\n",
    "for nn_num in range( model.n_local_pred):\n",
    "    scores = scores + raw_output[nn_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3])\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "#scores = scores[:tst_data.shape[0]]\n",
    "\n",
    "print (scores.shape)\n",
    "_, index = torch.max(scores,1)\n",
    "index = index.cpu().data.numpy()\n",
    "print ((index == b_targets).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02\n"
     ]
    }
   ],
   "source": [
    "print (model.accurancy(np.transpose(calm_test[400:600],(0,2,1)),np.zeros((200))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.cpu().data[0]), 'w')\n",
    "torch.save(model.state_dict(), ModelFile)\n",
    "ModelFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-ae6f5027f0d6>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-ae6f5027f0d6>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    KERAS_BACKEND=tensorflow python -c \"from keras import backend\"\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ddcf1b92dbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ]
    }
   ],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,Dense,Dropout, BatchNormalization, Input, InputLayer,Activation, Flatten\n",
    "from keras.activations import relu\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(500,31)))\n",
    "model.add(Conv1D(filters=256, kernel_size=3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(7500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Dense(1500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3,activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.00001), metrics=['accuracy'], loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('my_model.h5', monitor='val_loss', verbose=1, save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11369 samples, validate on 387 samples\n",
      "Epoch 1/20\n",
      "11369/11369 [==============================] - 43s 4ms/step - loss: 1.1930 - acc: 0.3927 - val_loss: 1.1673 - val_acc: 0.3643\n",
      "Epoch 2/20\n",
      "11369/11369 [==============================] - 13s 1ms/step - loss: 1.1168 - acc: 0.4398 - val_loss: 1.5102 - val_acc: 0.3101\n",
      "Epoch 3/20\n",
      "11369/11369 [==============================] - 10s 884us/step - loss: 1.0465 - acc: 0.5020 - val_loss: 2.5104 - val_acc: 0.3101\n",
      "Epoch 4/20\n",
      "11369/11369 [==============================] - 10s 885us/step - loss: 0.9564 - acc: 0.5541 - val_loss: 3.0716 - val_acc: 0.3049\n",
      "Epoch 5/20\n",
      "11369/11369 [==============================] - 10s 881us/step - loss: 0.8983 - acc: 0.5698 - val_loss: 5.7202 - val_acc: 0.3540\n",
      "Epoch 6/20\n",
      "11369/11369 [==============================] - 10s 884us/step - loss: 0.8318 - acc: 0.6025 - val_loss: 5.5917 - val_acc: 0.3566\n",
      "Epoch 7/20\n",
      "11369/11369 [==============================] - 10s 882us/step - loss: 0.8363 - acc: 0.5804 - val_loss: 3.2696 - val_acc: 0.3178\n",
      "Epoch 8/20\n",
      "11369/11369 [==============================] - 10s 881us/step - loss: 0.7756 - acc: 0.6138 - val_loss: 4.8340 - val_acc: 0.3075\n",
      "Epoch 9/20\n",
      "11369/11369 [==============================] - 10s 892us/step - loss: 0.7430 - acc: 0.6150 - val_loss: 10.4642 - val_acc: 0.3411\n",
      "Epoch 10/20\n",
      "11369/11369 [==============================] - 10s 887us/step - loss: 0.7128 - acc: 0.6283 - val_loss: 5.8804 - val_acc: 0.3385\n",
      "Epoch 11/20\n",
      "11369/11369 [==============================] - 10s 888us/step - loss: 0.6909 - acc: 0.6248 - val_loss: 1.6211 - val_acc: 0.3385\n",
      "Epoch 12/20\n",
      "11369/11369 [==============================] - 10s 892us/step - loss: 0.7025 - acc: 0.6254 - val_loss: 10.1421 - val_acc: 0.3437\n",
      "Epoch 13/20\n",
      "11369/11369 [==============================] - 10s 886us/step - loss: 0.6802 - acc: 0.6272 - val_loss: 10.6655 - val_acc: 0.3333\n",
      "Epoch 14/20\n",
      "11369/11369 [==============================] - 10s 888us/step - loss: 0.6668 - acc: 0.6306 - val_loss: 10.6829 - val_acc: 0.3333\n",
      "Epoch 15/20\n",
      "11369/11369 [==============================] - 10s 902us/step - loss: 0.6572 - acc: 0.6296 - val_loss: 10.6047 - val_acc: 0.3385\n",
      "Epoch 16/20\n",
      "11369/11369 [==============================] - 10s 893us/step - loss: 0.6580 - acc: 0.6340 - val_loss: 10.3514 - val_acc: 0.3411\n",
      "Epoch 17/20\n",
      "11369/11369 [==============================] - 10s 899us/step - loss: 0.6355 - acc: 0.6357 - val_loss: 3.6750 - val_acc: 0.3437\n",
      "Epoch 18/20\n",
      "11369/11369 [==============================] - 10s 889us/step - loss: 0.6274 - acc: 0.6322 - val_loss: 10.5764 - val_acc: 0.3385\n",
      "Epoch 19/20\n",
      "11369/11369 [==============================] - 10s 910us/step - loss: 0.6239 - acc: 0.6358 - val_loss: 5.8864 - val_acc: 0.2946\n",
      "Epoch 20/20\n",
      "11369/11369 [==============================] - 10s 889us/step - loss: 0.6301 - acc: 0.6395 - val_loss: 5.9183 - val_acc: 0.3101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f272b08ec90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x = data_set,\n",
    "          y = to_categorical(targets),\n",
    "          batch_size=256,\n",
    "          epochs=20,\n",
    "          validation_data = (val_data, to_categorical(val_targs))\n",
    "          \n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
