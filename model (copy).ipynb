{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import pandas\n",
    "import Queue\n",
    "import pyedflib\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "time = np.array([201,1401,2601,3801,5001,6201,7401,8601,9801,11001,12201,13401,14601,15801,17001,18201,19401,20601,21801,23001,24201,25401,26601,27801,29001,30201, 31401])\n",
    "dirs = [\"happy\", \"loss\", \"neut\"]\n",
    "q_data_set = Queue.Queue()\n",
    "q_targets = Queue.Queue()\n",
    "for k in np.arange(3):\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in walk(dirs[k]):\n",
    "        files.extend(filenames)\n",
    "        break\n",
    "    for instance, file  in enumerate(files):\n",
    "        f_read = pyedflib.EdfReader(dirs[k] +\"/\"+ file)\n",
    "        n = f_read.signals_in_file\n",
    "        signal_labels = f_read.getSignalLabels()\n",
    "        sigbufs = np.zeros((n, f_read.getNSamples()[0]))\n",
    "        shape = f_read.readSignal(0).shape[0]\n",
    "        data =  np.zeros((n-1, shape))\n",
    "        for i in np.arange(n-1):\n",
    "            data[i, :] = f_read.readSignal(i)\n",
    "        i = 0\n",
    "        while (i < 26 and time[i] < shape):\n",
    "            q_data_set.put(copy.deepcopy(data[:, time[i]:time[i+1]]))\n",
    "            q_targets.put(k)\n",
    "            i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = np.zeros((q_data_set.qsize() - 69, 62, 1200))\n",
    "targets = np.zeros((q_targets.qsize() - 69))\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while (not q_data_set.empty()):\n",
    "    temp = q_data_set.get_nowait()\n",
    "    temp_targ = q_targets.get_nowait()\n",
    "    if (temp.shape[1] != 1200 ):\n",
    "        continue\n",
    "    \n",
    "    data_set[j] = copy.deepcopy(temp[:62, :])\n",
    "    targets[j] = copy.deepcopy(temp_targ)\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(data_set.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "data_set = data_set[indexes]\n",
    "targets = targets[indexes]\n",
    "\n",
    "tst_data = np.zeros((data_set.shape[0]%256, 62, 1200))\n",
    "tst_targs = np.zeros((data_set.shape[0]%256))\n",
    "\n",
    "tst_data = copy.deepcopy(data_set[data_set.shape[0] - (data_set.shape[0]%256) :])\n",
    "tst_targs = copy.deepcopy(targets[targets.shape[0] - (targets.shape[0]%256) :])\n",
    "\n",
    "val_data = copy.deepcopy(data_set[:256])\n",
    "val_targs = copy.deepcopy(targets[:256])\n",
    "\n",
    "\n",
    "data_set = data_set[256:(data_set.shape[0] - (data_set.shape[0]%256))]\n",
    "targets = targets[256:(targets.shape[0] - (targets.shape[0]%256))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.shape[0]/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, targets, batch_size):\n",
    "    indexes = np.arange(dataset.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    dataset = dataset[indexes]\n",
    "    targets = targets[indexes]\n",
    "    \n",
    "    tr_data = np.zeros((dataset.shape[0]/batch_size, batch_size, 62, 1200))   \n",
    "    tr_targs = np.zeros((dataset.shape[0]/batch_size, batch_size))\n",
    "    \n",
    "    for i in range (dataset.shape[0]/batch_size):\n",
    "            \n",
    "        tr_data[i] = copy.deepcopy(dataset[i*batch_size:(i+1)*batch_size])\n",
    "        tr_targs[i] = copy.deepcopy(targets[i*batch_size: (i+1)*batch_size])\n",
    "        \n",
    "    return tr_data, tr_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LikeDislikeModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_size,\n",
    "                 L_sizes = [300, 200, 300],\n",
    "                 fc_size = 20,\n",
    "                 n_local_pred = 3,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [24, 16, 8], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True):\n",
    "        \n",
    "        super(LikeDislikeModel, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.fc_size = fc_size\n",
    "        self.out_size = out_size\n",
    "        self.n_local_pred = n_local_pred\n",
    "        self.use_cuda = use_cuda\n",
    "        self.L_sizes = L_sizes\n",
    "        \n",
    "        self.f_sizes = f_sizes\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.len_sizes = self._len_sizes()\n",
    "        \n",
    "        self.conv1 = nn.ModuleList()\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        self.conv3 = nn.ModuleList()\n",
    "        self.fc1 = nn.ModuleList()\n",
    "        self.fc2 = nn.ModuleList()\n",
    "        \n",
    "        # TODO : add batchnormalization for each conv \n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range (n_local_pred):\n",
    "            \n",
    "            self.conv1.append(nn.Conv1d(in_channels, channels[0], f_sizes[0], strides[0]))\n",
    "            self.conv2.append(nn.Conv1d(channels[0], channels[1], f_sizes[1], strides[1]))               \n",
    "            self.conv3.append(nn.Conv1d(channels[1], channels[2], f_sizes[2], strides[2]))\n",
    "            \n",
    "            self.fc1.append(nn.Linear(self.len_sizes[i][-1] * self.channels[-1], fc_size))\n",
    "            self.fc2.append(nn.Linear(fc_size, out_size))\n",
    "            self.batchnorm.append(nn.ModuleList())\n",
    "            for convNum in range(3):\n",
    "                self.batchnorm[i].append(nn.BatchNorm1d(channels[convNum]))\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def _len_sizes(self):\n",
    "        len_sizes = []\n",
    "        for i in range (self.n_local_pred):\n",
    "            temp = []\n",
    "            for j in range (len(self.L_sizes)):\n",
    "                if j == 0:\n",
    "                    temp.append(int(math.floor((self.L_sizes[i] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "                else:\n",
    "                    temp.append(int(math.floor((temp[j-1] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "            len_sizes.append(copy.deepcopy(temp))\n",
    "        return len_sizes\n",
    "\n",
    "    def preprocessing(self, input_batch):\n",
    "        nns = []\n",
    "        nns.append(input_batch[:, :, 200:500])\n",
    "        nns.append(input_batch[:, :, 500:700])\n",
    "        nns.append(input_batch[:, :, 700:1000])\n",
    "        for i in range (self.n_local_pred):\n",
    "            zero_mean = nns[i] - nns[i].mean()\n",
    "            for n in range(input_batch.shape[0]):\n",
    "                nns[i][n] = zero_mean[n, :, :]/(np.std(zero_mean[n, :, :],axis = 0))\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i]))).cuda()\n",
    "            else:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i])))\n",
    "        return nns\n",
    "\n",
    "    def createLocalNetworks(self, nns):\n",
    "        raw_output = []\n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            conv1 = self.conv1[nn_num](nns[nn_num])\n",
    "            bn1 = self.batchnorm[nn_num][0](conv1)\n",
    "            relu1 = F.relu(bn1)\n",
    "\n",
    "            conv2 = self.conv2[nn_num](relu1)\n",
    "            bn2 = self.batchnorm[nn_num][1](conv2)\n",
    "            relu2 = F.relu(bn2)\n",
    "            \n",
    "            conv3 = self.conv3[nn_num](relu2)\n",
    "            bn3 = self.batchnorm[nn_num][2](conv3)\n",
    "            relu3 = F.relu(bn3)\n",
    "            \n",
    "            relu3 = relu3.view(self.batch_size, -1)\n",
    "            \n",
    "            fc1 = self.fc1[nn_num](relu3)\n",
    "            relu4 = F.relu(fc1)\n",
    "\n",
    "            do = self.dropout(relu4)\n",
    "\n",
    "            fc2 = self.fc2[nn_num](do)\n",
    "            \n",
    "            raw_output.append(F.log_softmax(fc2, dim = 1))\n",
    "        return raw_output\n",
    "    \n",
    "    def _loss(self, raw_output, targets):\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "            \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "        \n",
    "        \n",
    "        if self.use_cuda:\n",
    "            targets = Variable(torch.LongTensor(targets)).cuda()\n",
    "        else:\n",
    "            targets = Variable(torch.LongTensor(targets))\n",
    "            \n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        return loss\n",
    "    \n",
    "    def accurancy(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "            \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "            \n",
    "        _, index = torch.max(scores,1)\n",
    "        index = index.cpu().data.numpy()\n",
    "        acc = (index == targets).mean()\n",
    "        return acc\n",
    "    \n",
    "    def forward(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        loss = self._loss(raw_output, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir_root = './logdir'\n",
    "model = LikeDislikeModel(62, 3, use_cuda = True)\n",
    "\n",
    "if model.use_cuda:\n",
    "    model.cuda()\n",
    "#model.load_state_dict(torch.load(logdir_root +  '/block99-loss=0.365_model.txt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.00002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 0, loss = 1.048, loss = 1.051, val_acc = 0.383, train_acc = 0.434\n",
      "block 1, loss = 1.054, loss = 1.049, val_acc = 0.375, train_acc = 0.379\n",
      "block 2, loss = 1.044, loss = 1.047, val_acc = 0.383, train_acc = 0.438\n",
      "block 3, loss = 1.043, loss = 1.042, val_acc = 0.391, train_acc = 0.406\n",
      "block 4, loss = 1.035, loss = 1.045, val_acc = 0.387, train_acc = 0.406\n",
      "block 5, loss = 1.009, loss = 1.031, val_acc = 0.395, train_acc = 0.371\n"
     ]
    }
   ],
   "source": [
    "logdir_root = './logdir2'\n",
    "LossFile = open(logdir_root+'/loss.txt', 'w')\n",
    "ValLossFile = open(logdir_root+'/val_loss.txt', 'w')\n",
    "\n",
    "iterations = 7\n",
    "epochs = 10000\n",
    "\n",
    "log_loss = np.zeros(epochs*iterations)\n",
    "block = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tr_data, tr_targs = get_data(data_set, targets, 256)\n",
    "    indexes = np.arange(iterations)\n",
    "    np.random.shuffle(indexes)\n",
    "    tr_data = tr_data[indexes]\n",
    "    tr_targs = tr_targs[indexes]\n",
    "    for iteration in range(iterations):\n",
    "        batch = tr_data[iteration]\n",
    "        b_targets = tr_targs[iteration]\n",
    "        b_indexes = np.arange(256)\n",
    "        np.random.shuffle(b_indexes)\n",
    "        batch = batch[b_indexes]\n",
    "        b_targets = b_targets[b_indexes]\n",
    "        model.zero_grad()\n",
    "        loss = model(batch, b_targets)\n",
    "        log_loss[epoch*iterations + iteration] = loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        if model.use_cuda:\n",
    "            v_acc = model.accurancy(val_data, val_targs)\n",
    "            acc =  model.accurancy(val_data, val_targs)\n",
    "            v_loss = model(batch, b_targets)\n",
    "            print('block {:d}, loss = {:.3f}, loss = {:.3f}, val_acc = {:.3f}, train_acc = {:.3f}'\n",
    "                  .format(block, loss.cpu().data[0], v_loss.cpu().data[0], v_acc, acc))\n",
    "            ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.cpu().data[0]), 'w')\n",
    "            ValLossFile.write('{:.3f} {:.3f}\\n'.format(acc, v_acc))\n",
    "        else:\n",
    "            print('block {:d}, loss = {:.3f}'\n",
    "                  .format(block, loss.data[0]))\n",
    "            ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.data[0]), 'w')\n",
    "            ValLossFile.write('{:.3f} {:.3f}\\n'.format(loss.data[0], v_loss.data[0]))\n",
    "        block += 1\n",
    "    \n",
    "        torch.save(model.state_dict(), ModelFile)\n",
    "        ModelFile.close()\n",
    "    if epoch % 400 == 0:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr/1.5)\n",
    "np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 62, 1200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.zeros((256, 62, 1200))\n",
    "for i in range (tst_data.shape[0]):\n",
    "    test[i] = tst_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nns = model.preprocessing(batch)\n",
    "raw_output = model.createLocalNetworks(nns)\n",
    "scores = Variable(torch.FloatTensor(model.batch_size, model.out_size).zero_()).cuda()\n",
    "for nn_num in range( model.n_local_pred):\n",
    "    scores = scores + raw_output[nn_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3])\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "#scores = scores[:tst_data.shape[0]]\n",
    "\n",
    "print (scores.shape)\n",
    "_, index = torch.max(scores,1)\n",
    "index = index.cpu().data.numpy()\n",
    "print ((index == b_targets).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 1 1 2 2 1 2 2 1 0 2 0 0 0 2 2 1 0 0 0 0 0 2 2 1 1 2 1 1 2 0 0 2 2 0 0\n",
      " 0 2 0 1 1 1 0 2 1 2 1 2 1 0 2 1 0 2 0 2 0 0 1 1 0 1 1 2 2 2 1 2 1 2 1 1 2\n",
      " 2 1 0 0 1 2 2 2 2 2 1 0 1 1 0 1 2 0 1 2 1 1 2 0 0 0 2 2 0 2 1 0 1 0 2 2 1\n",
      " 1 1 2 0 1 2 0 0 2 1 0 0 1 1 1 0 2 1 1 1 1 1 2 2 0 1 1 2 2 0 0 1 0 1 1 1 1\n",
      " 0 0 2 1 2 2 2 0 1 0 1 0 2 2 2 1 1 0 1 0 0 2 1 2 0 1 1 1 2 2 1] [ 2.  0.  1.  1.  2.  2.  1.  2.  2.  1.  0.  2.  0.  0.  0.  2.  2.  1.\n",
      "  0.  0.  1.  0.  0.  2.  0.  1.  0.  2.  1.  1.  2.  0.  0.  2.  2.  0.\n",
      "  0.  0.  2.  0.  1.  1.  1.  0.  2.  1.  2.  0.  1.  1.  0.  2.  1.  0.\n",
      "  2.  0.  2.  0.  0.  1.  1.  0.  1.  1.  1.  0.  2.  1.  2.  1.  2.  1.\n",
      "  1.  2.  2.  1.  0.  0.  1.  2.  1.  1.  2.  2.  1.  0.  1.  1.  0.  1.\n",
      "  2.  0.  1.  2.  1.  1.  2.  0.  1.  0.  2.  2.  0.  2.  1.  0.  1.  0.\n",
      "  2.  0.  1.  1.  1.  2.  0.  1.  2.  0.  0.  2.  1.  0.  0.  1.  1.  1.\n",
      "  0.  2.  2.  1.  1.  1.  2.  2.  1.  0.  1.  1.  2.  2.  0.  0.  1.  0.\n",
      "  1.  0.  1.  1.  0.  0.  2.  0.  2.  2.  2.  0.  1.  0.  2.  0.  2.  2.\n",
      "  1.  1.  1.  0.  1.  0.  0.  2.  1.  2.  0.  2.  0.  1.  2.  2.  1.]\n",
      "0.888268156425\n"
     ]
    }
   ],
   "source": [
    "print (model.accurancy(tst_data, tst_targs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
