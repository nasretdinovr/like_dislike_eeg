{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import pandas\n",
    "import Queue\n",
    "import pyedflib\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dir = \"np_data13\"\n",
    "files = []\n",
    "data_set_t = []\n",
    "target_t = []\n",
    "\n",
    "for (dirpath, dirnames, filenames) in walk(dir):\n",
    "    files.extend(filenames)\n",
    "    break\n",
    "for instance, file  in enumerate(files):\n",
    "    if (file == \"T0.npy\" or file == \"T1.npy\" or file == \"T2.npy\" or file == \"T3.npy\" or file == \"T4.npy\" or file == \"T5.npy\"):\n",
    "        continue\n",
    "    if (file == \"0.npy\" or file == \"1.npy\" or file == \"2.npy\" or file == \"3.npy\" or file == \"4.npy\" or file == \"5.npy\"):\n",
    "        data_set_t.append(np.load(dir + '/' +file))\n",
    "        target_t.append(np.load(dir + '/T' +file))\n",
    "calm_test= np.load(dir + '/calm_test.npy')\n",
    "calm_train = np.load(dir + '/calm_train.npy')\n",
    "\n",
    "Tcalm_test= np.load(dir + '/Tcalm_test.npy')\n",
    "Tcalm_train = np.load(dir + '/Tcalm_train.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8628,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"temp = data_set_t[0]\n",
    "for i in range (5):\n",
    "    temp = np.vstack(data_set_t[i+1])\"\"\"\n",
    "data_set = np.vstack((data_set_t[0],data_set_t[1],data_set_t[2],data_set_t[3],data_set_t[4], data_set_t[5]))\n",
    "targets = np.hstack((target_t[0],target_t[1],target_t[2],target_t[3],target_t[4],  target_t[5]))\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8628 % 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8448\n"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(data_set.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "data_set = data_set[indexes]\n",
    "targets = targets[indexes]\n",
    "\n",
    "\"\"\"tst_data = np.zeros((data_set.shape[0]%256, 62, 1200))\n",
    "tst_targs = np.zeros((data_set.shape[0]%256))\"\"\"\n",
    "\n",
    "tst_data = copy.deepcopy(data_set[data_set.shape[0] - (data_set.shape[0]%256) :])\n",
    "tst_targs = copy.deepcopy(targets[targets.shape[0] - (targets.shape[0]%256) :])\n",
    "\n",
    "val_data = copy.deepcopy(data_set[:256])\n",
    "val_targs = copy.deepcopy(targets[:256])\n",
    "print (tst_data.shape)\n",
    "\n",
    "data_set = data_set[256:(data_set.shape[0] - (data_set.shape[0]%256))]\n",
    "targets = targets[256:(targets.shape[0] - (targets.shape[0]%256))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(data_set.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "data_set = data_set[indexes]\n",
    "targets = targets[indexes]\n",
    "\n",
    "\"\"\"tst_data = np.zeros((data_set.shape[0]%256, 62, 1200))\n",
    "tst_targs = np.zeros((data_set.shape[0]%256))\"\"\"\n",
    "\n",
    "tst_data = copy.deepcopy(data_set[data_set.shape[0] - (data_set.shape[0]%256) :, 150:250])\n",
    "tst_targs = copy.deepcopy(targets[targets.shape[0] - (targets.shape[0]%256) :])\n",
    "\n",
    "val_data = copy.deepcopy(data_set[:256, 150:250])\n",
    "val_targs = copy.deepcopy(targets[:256])\n",
    "print (data_set.shape[0] - (data_set.shape[0]%256))\n",
    "\n",
    "data_set = data_set[256:(data_set.shape[0] - (data_set.shape[0]%256)), 150:250]\n",
    "targets = targets[256:(targets.shape[0] - (targets.shape[0]%256))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_targs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 500, 31)\n",
      "(384,)\n",
      "(384, 500, 31)\n"
     ]
    }
   ],
   "source": [
    "tst_data = np.vstack((calm_test[:120,], tst_data))\n",
    "\n",
    "tst_data.shape\n",
    "\n",
    "tst_targs = np.hstack((Tcalm_train[:120], tst_targs))\n",
    "print (tst_targs.shape, tst_data.shape)\n",
    "val_data = np.vstack((calm_test[120:248], val_data))\n",
    "\n",
    "val_targs = np.hstack((Tcalm_test[120:248], val_targs))\n",
    "print ( val_targs.shape)\n",
    "print (val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = np.transpose(val_data, (0, 2, 1))\n",
    "tst_data = np.transpose(tst_data, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = np.vstack((data_set, calm_train))\n",
    "targets = np.hstack((targets, Tcalm_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = np.transpose(data_set, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11192, 31, 500) (11192,)\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "print (data_set.shape,targets.shape )\n",
    "print (10424 % 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7919"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7424+256+239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.shape[0]/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, targets, batch_size):\n",
    "    indexes = np.arange(dataset.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    dataset = dataset[indexes]\n",
    "    targets = targets[indexes]\n",
    "    \n",
    "    tr_data = np.zeros((dataset.shape[0]/batch_size, batch_size, 31, 500))   \n",
    "    tr_targs = np.zeros((dataset.shape[0]/batch_size, batch_size))\n",
    "    \n",
    "    for i in range (dataset.shape[0]/batch_size):\n",
    "            \n",
    "        tr_data[i] = copy.deepcopy(dataset[i*batch_size:(i+1)*batch_size])\n",
    "        tr_targs[i] = copy.deepcopy(targets[i*batch_size: (i+1)*batch_size])\n",
    "        \n",
    "    return tr_data, tr_targs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LikeDislikeModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,\n",
    "                 out_size,\n",
    "                 L_sizes = [300, 200, 300],\n",
    "                 net = None,\n",
    "                 fc_sizes = [700, 250],\n",
    "                 n_local_pred = 3,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [256, 128, 64], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True):\n",
    "        \n",
    "        super(LikeDislikeModel, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        self.fc_sizes = fc_sizes\n",
    "        self.out_size = out_size\n",
    "        self.n_local_pred = n_local_pred\n",
    "        self.use_cuda = use_cuda\n",
    "        self.L_sizes = L_sizes\n",
    "        self.net = net\n",
    "        \n",
    "        self.f_sizes = f_sizes\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.len_sizes = self._len_sizes()\n",
    "        \n",
    "        self.conv1 = nn.ModuleList()\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        self.conv3 = nn.ModuleList()\n",
    "        self.fc1 = nn.ModuleList()\n",
    "        self.fc2 = nn.ModuleList()\n",
    "        self.fc3 = nn.ModuleList()\n",
    "        \n",
    "        # TODO : add batchnormalization for each conv \n",
    "        self.batchnorm = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        for i in range (n_local_pred):\n",
    "            \n",
    "            self.conv1.append(nn.Conv1d(in_channels, channels[0], f_sizes[0], strides[0]))\n",
    "            self.conv2.append(nn.Conv1d(channels[0], channels[1], f_sizes[1], strides[1]))               \n",
    "            self.conv3.append(nn.Conv1d(channels[1], channels[2], f_sizes[2], strides[2]))\n",
    "            \n",
    "            self.fc1.append(nn.Linear(self.len_sizes[i][-1] * self.channels[-1], fc_sizes[0]))\n",
    "            self.fc2.append(nn.Linear(fc_sizes[0], fc_sizes[1]))\n",
    "            self.fc3.append(nn.Linear(fc_sizes[1], out_size))\n",
    "            self.batchnorm.append(nn.ModuleList())\n",
    "            for convNum in range(3):\n",
    "                self.batchnorm[i].append(nn.BatchNorm1d(channels[convNum]))\n",
    "            \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def _len_sizes(self):\n",
    "        len_sizes = []\n",
    "        for i in range (self.n_local_pred):\n",
    "            temp = []\n",
    "            for j in range (len(self.f_sizes)):\n",
    "                if j == 0:\n",
    "                    temp.append(int(math.floor((self.L_sizes[i] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "                else:\n",
    "                    temp.append(int(math.floor((temp[j-1] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "            len_sizes.append(copy.deepcopy(temp))\n",
    "        return len_sizes\n",
    "\n",
    "    def preprocessing(self, input_batch):\n",
    "        nns = []\n",
    "        if (self.n_local_pred == 1):\n",
    "            if (self.net == 0):\n",
    "                nns.append(input_batch[:, :, 200:500])\n",
    "            elif (self.net == 1):\n",
    "                nns.append(input_batch[:, :, 500:700])\n",
    "            elif (self.net == 2):\n",
    "                nns.append(input_batch[:, :, 700:1000])\n",
    "            elif (self.net == 3):\n",
    "                nns.append(input_batch)\n",
    "            else:\n",
    "                nns.append(input_batch[:, :, 200:1000])\n",
    "        else:\n",
    "            nns.append(input_batch[:, :, 200:500])\n",
    "            nns.append(input_batch[:, :, 500:700])\n",
    "            nns.append(input_batch[:, :, 700:1000])\n",
    "        #print (nns[0].shape, len(nns))\n",
    "        for i in range (self.n_local_pred):\n",
    "            zero_mean = nns[i] - nns[i].mean()\n",
    "            for n in range(input_batch.shape[0]):\n",
    "                nns[i][n] = zero_mean[n, :, :]/(np.std(zero_mean[n, :, :],axis = 0))\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i]))).cuda()\n",
    "            else:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i])))\n",
    "        return nns\n",
    "\n",
    "    def createLocalNetworks(self, nns):\n",
    "        raw_output = []\n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            conv1 = self.conv1[nn_num](nns[nn_num])\n",
    "            bn1 = self.batchnorm[nn_num][0](conv1)\n",
    "            relu1 = F.relu(bn1)\n",
    "\n",
    "            conv2 = self.conv2[nn_num](relu1)\n",
    "            bn2 = self.batchnorm[nn_num][1](conv2)\n",
    "            relu2 = F.relu(bn2)\n",
    "            \n",
    "            conv3 = self.conv3[nn_num](relu2)\n",
    "            bn3 = self.batchnorm[nn_num][2](conv3)\n",
    "            relu3 = F.relu(bn3)\n",
    "            \n",
    "            relu3 = relu3.view(self.batch_size, -1)\n",
    "            #print (relu3.size(), self.len_sizes[i][-1] * self.channels[-1]) \n",
    "            #print (self.len_sizes[0], len(self.len_sizes))\n",
    "            \n",
    "            fc1 = self.fc1[nn_num](relu3)\n",
    "            relu4 = F.relu(fc1)\n",
    "            do1 = self.dropout(relu4)\n",
    "            \n",
    "            fc2 = self.fc2[nn_num](do1)\n",
    "            relu5 = F.relu(fc2)            \n",
    "\n",
    "            do2 = self.dropout(relu5)\n",
    "\n",
    "            fc3 = self.fc3[nn_num](do2)\n",
    "            \n",
    "            raw_output.append(F.log_softmax(fc3, dim = 1))\n",
    "        return raw_output\n",
    "    \n",
    "    def _loss(self, raw_output, targets):\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "            \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "        scores = scores/self.n_local_pred\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            targets = Variable(torch.LongTensor(targets)).cuda()\n",
    "        else:\n",
    "            targets = Variable(torch.LongTensor(targets))\n",
    "            \n",
    "        loss = F.cross_entropy(scores, targets)\n",
    "        return loss\n",
    "    \n",
    "    def accurancy(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        \n",
    "        if self.use_cuda:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_()).cuda()\n",
    "        else:\n",
    "            scores = Variable(torch.FloatTensor(self.batch_size, self.out_size).zero_())\n",
    "        \n",
    "    \n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            scores = scores + raw_output[nn_num]\n",
    "        scores = scores/self.n_local_pred\n",
    "            \n",
    "        _, index = torch.max(scores,1)\n",
    "        index = index.cpu().data.numpy()\n",
    "        acc = (index == targets).mean()\n",
    "        return acc\n",
    "    \n",
    "    def forward(self, input_batch, targets):\n",
    "        self.batch_size = input_batch.shape[0]\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        raw_output = self.createLocalNetworks(nns)\n",
    "        loss = self._loss(raw_output, targets)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir_root = './logdir'\n",
    "model = LikeDislikeModel(31, 3, L_sizes = [500],\n",
    "                 net = 3,\n",
    "                 fc_sizes = [7500, 1500],\n",
    "                 n_local_pred = 1,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [256, 128, 64], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True)\n",
    "\n",
    "if model.use_cuda:\n",
    "    model.cuda()\n",
    "#model.load_state_dict(torch.load(logdir_root +  '/block0-loss=0.499_model.txt'))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001, weight_decay = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000009, weight_decay = 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss = 1.076, val_loss = 1.082, val_acc = 0.391, train_acc = 0.449\n",
      "epoch 1, loss = 1.033, val_loss = 1.069, val_acc = 0.404, train_acc = 0.465\n",
      "epoch 2, loss = 0.994, val_loss = 1.031, val_acc = 0.396, train_acc = 0.477\n",
      "epoch 3, loss = 0.855, val_loss = 0.850, val_acc = 0.680, train_acc = 0.598\n",
      "epoch 4, loss = 0.674, val_loss = 0.639, val_acc = 0.669, train_acc = 0.629\n",
      "epoch 5, loss = 0.660, val_loss = 0.594, val_acc = 0.677, train_acc = 0.594\n",
      "epoch 6, loss = 0.637, val_loss = 0.575, val_acc = 0.685, train_acc = 0.613\n",
      "epoch 7, loss = 0.594, val_loss = 0.574, val_acc = 0.682, train_acc = 0.660\n",
      "epoch 8, loss = 0.595, val_loss = 0.558, val_acc = 0.688, train_acc = 0.629\n",
      "epoch 9, loss = 0.593, val_loss = 0.562, val_acc = 0.685, train_acc = 0.652\n",
      "epoch 10, loss = 0.570, val_loss = 0.556, val_acc = 0.677, train_acc = 0.652\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-eb79cb7296b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mb_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_indexes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlog_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0miterations\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ffdad5ca6b64>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_batch, targets)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mnns\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateLocalNetworks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ffdad5ca6b64>\u001b[0m in \u001b[0;36mpreprocessing\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rauf/.conda/envs/my_root/lib/python2.7/copy.pyc\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__deepcopy__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logdir_root = './logdir2'\n",
    "#ValLossFile = open(logdir_root+'/val_loss.txt', 'w')\n",
    "\n",
    "iterations = 43\n",
    "epochs = 1000\n",
    "\n",
    "log_loss = np.zeros(epochs*iterations)\n",
    "block = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tr_data, tr_targs = get_data(data_set, targets, 256)\n",
    "    indexes = np.arange(iterations)\n",
    "    np.random.shuffle(indexes)\n",
    "    tr_data = tr_data[indexes]\n",
    "    tr_targs = tr_targs[indexes]\n",
    "    for iteration in range(iterations):\n",
    "        batch = tr_data[iteration]\n",
    "        b_targets = tr_targs[iteration]\n",
    "        b_indexes = np.arange(256)\n",
    "        np.random.shuffle(b_indexes)\n",
    "        batch = batch[b_indexes]\n",
    "        b_targets = b_targets[b_indexes]\n",
    "        model.zero_grad()\n",
    "        loss = model(batch, b_targets)\n",
    "        log_loss[epoch*iterations + iteration] = loss.data\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #if epoch % 10 == 0:\n",
    "    \n",
    "    if model.use_cuda:\n",
    "        v_acc = model.accurancy(val_data, val_targs)\n",
    "        acc =  model.accurancy(batch, b_targets)\n",
    "        v_loss = model(val_data, val_targs)\n",
    "        print('epoch {:d}, loss = {:.3f}, val_loss = {:.3f}, val_acc = {:.3f}, train_acc = {:.3f}'\n",
    "              .format(epoch, loss.cpu().data[0], v_loss.cpu().data[0], v_acc, acc))\n",
    "        ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.cpu().data[0]), 'w')\n",
    "        #ValLossFile.write('{:.3f} {:.3f}\\n'.format(acc, v_acc))\n",
    "    else:\n",
    "        print('block {:d}, loss = {:.3f}'\n",
    "              .format(block, loss.data[0]))\n",
    "        ModelFile = open(logdir_root+'/block{:d}-loss={:.3f}_model.txt'.format(block, loss.data[0]), 'w')\n",
    "        #ValLossFile.write('{:.3f} {:.3f}\\n'.format(loss.data[0], v_loss.data[0]))\n",
    "   # block += 1\n",
    "\n",
    "    torch.save(model.state_dict(), ModelFile)\n",
    "    ModelFile.close()\n",
    "#np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('log_loss.txt', log_loss, fmt='%1.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179, 62, 1200)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.zeros((256, 62, 1200))\n",
    "for i in range (tst_data.shape[0]):\n",
    "    test[i] = tst_data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nns = model.preprocessing(batch)\n",
    "raw_output = model.createLocalNetworks(nns)\n",
    "scores = Variable(torch.FloatTensor(model.batch_size, model.out_size).zero_()).cuda()\n",
    "for nn_num in range( model.n_local_pred):\n",
    "    scores = scores + raw_output[nn_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 3])\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "#scores = scores[:tst_data.shape[0]]\n",
    "\n",
    "print (scores.shape)\n",
    "_, index = torch.max(scores,1)\n",
    "index = index.cpu().data.numpy()\n",
    "print ((index == b_targets).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773333333333\n"
     ]
    }
   ],
   "source": [
    "print (model.accurancy(tst_data, tst_targs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
