{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from __future__ import print_function\n",
    "from torch import nn\n",
    "from torch import autograd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pyedflib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5a08021da950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyedflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyedflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEdfReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"edf1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignals_in_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msignal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetSignalLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pyedflib"
     ]
    }
   ],
   "source": [
    "\"\"\"\"#import pyedflib\n",
    "import numpy as np\n",
    "f = pyedflib.EdfReader(\"edf1\")\n",
    "n = f.signals_in_file\n",
    "signal_labels = f.getSignalLabels()\n",
    "sigbufs = np.zeros((n, f.getNSamples()[0]))\n",
    "for i in np.arange(n):\n",
    "    sigbufs[i, :] = f.readSignal(i)\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = sigbufs[:, 201:1401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LikeDislikeModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 batch_size,\n",
    "                 in_channels,\n",
    "                 out_size,\n",
    "                 L_sizes = [300, 200, 300],\n",
    "                 fc_size = 20,\n",
    "                 n_local_pred = 3,\n",
    "                 f_sizes = [3, 3, 5], \n",
    "                 channels = [24, 16, 8], \n",
    "                 strides = [2, 2, 3], \n",
    "                 use_cuda = True):\n",
    "        \n",
    "        super(LikeDislikeModel, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.batch_size = batch_size\n",
    "        self.fc_size = fc_size\n",
    "        self.out_size = out_size\n",
    "        self.n_local_pred = n_local_pred\n",
    "        self.use_cuda = use_cuda\n",
    "        self.L_sizes = L_sizes\n",
    "        \n",
    "        self.f_sizes = f_sizes\n",
    "        self.channels = channels\n",
    "        self.strides = strides\n",
    "        self.len_sizes = self._len_sizes()\n",
    "        \n",
    "        self.conv1 = nn.ModuleList()\n",
    "        self.conv2 = nn.ModuleList()\n",
    "        self.conv3 = nn.ModuleList()\n",
    "        self.fc1 = nn.ModuleList()\n",
    "        self.fc2 = nn.ModuleList()\n",
    "        \n",
    "        # TODO : add batchnormalization for each conv \n",
    "        #self.batchnorm = nn.ModuleList()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range (n_local_pred):\n",
    "            \n",
    "            self.conv1.append(nn.Conv1d(in_channels, channels[0], f_sizes[0], strides[0]))\n",
    "            self.conv2.append(nn.Conv1d(channels[0], channels[1], f_sizes[1], strides[1]))               \n",
    "            self.conv3.append(nn.Conv1d(channels[1], channels[2], f_sizes[2], strides[2]))\n",
    "            \n",
    "            self.fc1.append(nn.Linear(self.len_sizes[i][-1] * self.channels[-1], fc_size))\n",
    "            self.fc2.append(nn.Linear(fc_size, out_size))\n",
    "            \n",
    "            \n",
    "                              \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def _len_sizes(self):\n",
    "        len_sizes = []\n",
    "        for i in range (self.n_local_pred):\n",
    "            temp = []\n",
    "            for j in range (len(self.L_sizes)):\n",
    "                if j == 0:\n",
    "                    temp.append(int(math.floor((self.L_sizes[i] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "                else:\n",
    "                    temp.append(int(math.floor((temp[j-1] - self.f_sizes[j])/self.strides[j] + 1)))\n",
    "            len_sizes.append(copy.deepcopy(temp))\n",
    "        return len_sizes\n",
    "\n",
    "    def preprocessing(self, input_batch):\n",
    "        nns = []\n",
    "        nns.append(input_batch[:, :, 200:500])\n",
    "        nns.append(input_batch[:, :, 500:700])\n",
    "        nns.append(input_batch[:, :, 700:1000])\n",
    "        for i in range (self.n_local_pred):\n",
    "            zero_mean = nns[i] - nns[i].mean()\n",
    "            for n in range(input_batch.shape[0]):\n",
    "                nns[i][n] = zero_mean[n, :, :]/(np.std(zero_mean[n, :, :],axis = 0) + 0.1)\n",
    "                \n",
    "            if self.use_cuda:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i]))).cuda()\n",
    "            else:\n",
    "                nns[i] = Variable(torch.FloatTensor(copy.deepcopy(nns[i])))\n",
    "                \n",
    "\n",
    "        return nns\n",
    "\n",
    "    def createLocalNetworks(self, nns):\n",
    "        predict = []\n",
    "        for nn_num in range( self.n_local_pred):\n",
    "            conv1 = self.conv1[nn_num](nns[nn_num])\n",
    "            relu1 = F.relu(conv1)\n",
    "            print (conv1.size())\n",
    "\n",
    "            conv2 = self.conv2[nn_num](relu1)\n",
    "            relu2 = F.relu(conv2)\n",
    "            print (conv2.size())\n",
    "            \n",
    "            conv3 = self.conv3[nn_num](relu2)\n",
    "            relu3 = F.relu(conv3)\n",
    "            print (conv2.size())\n",
    "            \n",
    "            relu3 = relu3.view(self.batch_size, -1)\n",
    "            \n",
    "            fc1 = self.fc1[nn_num](relu3)\n",
    "            relu4 = F.relu(fc1)\n",
    "\n",
    "            do = self.dropout(relu4)\n",
    "\n",
    "            fc2 = self.fc2[nn_num](do)\n",
    "            \n",
    "            predict.append(F.log_softmax(fc2))\n",
    "        return predict\n",
    "\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        print (123)\n",
    "        nns  = self.preprocessing(input_batch)\n",
    "        predict = self.createLocalNetworks(nns)\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "torch.Size([5, 24, 149])\n",
      "torch.Size([5, 16, 74])\n",
      "torch.Size([5, 16, 74])\n",
      "torch.Size([5, 24, 99])\n",
      "torch.Size([5, 16, 49])\n",
      "torch.Size([5, 16, 49])\n",
      "torch.Size([5, 24, 149])\n",
      "torch.Size([5, 16, 74])\n",
      "torch.Size([5, 16, 74])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rauf/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:108: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "model = LikeDislikeModel(5, 65, 3, use_cuda = False)\n",
    "if model.use_cuda:\n",
    "    model.cuda()\n",
    "output = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       " -1.1709 -1.1319 -1.0011\n",
       " -1.1730 -1.1416 -0.9908\n",
       " -1.1717 -1.1321 -1.0002\n",
       " -1.1725 -1.1409 -0.9919\n",
       " -1.1769 -1.1461 -0.9838\n",
       " [torch.FloatTensor of size 5x3], Variable containing:\n",
       " -1.1959 -0.9043 -1.2285\n",
       " -1.1745 -0.9016 -1.2549\n",
       " -1.1669 -0.9157 -1.2433\n",
       " -1.1571 -0.9256 -1.2403\n",
       " -1.1541 -0.9103 -1.2650\n",
       " [torch.FloatTensor of size 5x3], Variable containing:\n",
       " -1.2286 -1.1004 -0.9819\n",
       " -1.2012 -1.1221 -0.9846\n",
       " -1.2127 -1.1524 -0.9500\n",
       " -1.1965 -1.1335 -0.9786\n",
       " -1.1968 -1.1083 -1.0004\n",
       " [torch.FloatTensor of size 5x3]]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
